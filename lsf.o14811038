Sender: LSF System <lsfadmin@lo-s4-034>
Subject: Job 14811038: <./run.sh> in cluster <leonhard> Done

Job <./run.sh> was submitted from host <lo-login-02> by user <fanconic> in cluster <leonhard> at Wed Mar 10 23:11:29 2021
Job was executed on host(s) <4*lo-s4-034>, in queue <gpu.24h>, as user <fanconic> in cluster <leonhard> at Wed Mar 10 23:11:33 2021
</cluster/home/fanconic> was used as the home directory.
</cluster/home/fanconic/ML4H_project1> was used as the working directory.
Started at Wed Mar 10 23:11:33 2021
Terminated at Wed Mar 10 23:12:42 2021
Results reported at Wed Mar 10 23:12:42 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
./run.sh
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   459.12 sec.
    Max Memory :                                 15937 MB
    Average Memory :                             9757.60 MB
    Total Requested Memory :                     16384.00 MB
    Delta Memory :                               447.00 MB
    Max Swap :                                   -
    Max Processes :                              44
    Max Threads :                                120
    Run time :                                   76 sec.
    Turnaround time :                            73 sec.

The output (if any) follows:

Standard Training.
Number of files:  100
/cluster/home/fanconic/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 40 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
description:   0%|          | 0/27 [00:00<?, ?it/s]Traceback (most recent call last):
  File "train.py", line 105, in <module>
    output = model(x_train)
  File "/cluster/home/fanconic/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/fanconic/ML4H_project1/src/model/unet.py", line 81, in forward
    x2 = self.down_block2(x1)
  File "/cluster/home/fanconic/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/fanconic/ML4H_project1/src/model/unet.py", line 39, in forward
    x = self.down(x)
  File "/cluster/home/fanconic/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/fanconic/.local/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/cluster/home/fanconic/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/fanconic/ML4H_project1/src/model/unet.py", line 24, in forward
    x = self.conv(x)
  File "/cluster/home/fanconic/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/fanconic/.local/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/cluster/home/fanconic/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/fanconic/.local/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py", line 140, in forward
    self.weight, self.bias, bn_training, exponential_average_factor, self.eps)
  File "/cluster/home/fanconic/.local/lib/python3.7/site-packages/torch/nn/functional.py", line 2147, in batch_norm
    input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: CUDA out of memory. Tried to allocate 774.00 MiB (GPU 0; 10.92 GiB total capacity; 9.76 GiB already allocated; 411.38 MiB free; 9.76 GiB reserved in total by PyTorch)
Script completed!
